{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tabla Abstracts puesta a disposici ́on en el campus virtual tiene 2 colum-\n",
    "nas: department, y abstract. La columna abstract contiene el resumen de una\n",
    "publicaci ́on cient ́ıfica y la columna department indica un departamento univer-\n",
    "sitario asociado a esa publicaci ́on (con tres valores posibles).\n",
    "Las separaciones aleatorias de datos de las preguntas 1 y 2 deberan realizarse\n",
    "utilizando la semilla asignada 70."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta 1 (2 pts)**\n",
    "Preparar los datos para ajustar un Modelo de Clasificaci ́on. Para esto:\n",
    "1. Generar un DataFrame de Pandas para el conjunto de entrenamiento y\n",
    "otro para el conjunto de test (el conjunto test debe contener el 20% de los\n",
    "datos, con una distribución de clases identica a la original).\n",
    "\n",
    "2. Implementar una función de preprocesamiento y aplicarla sobre el conjunto de datos de entrenamiento. Se deben incluir al menos los siguientes\n",
    "métodos de limpieza en la implementaci ́on: \n",
    "* (i) Eliminar los caracteres que no sean letras del abecedario. \n",
    "* (ii) Estandarizar la escritura a letras minusculas. \n",
    "* (iii) Eliminar las t ́ıldes y los espacios en blanco sobrantes.\n",
    "* (iv) Eliminar las stopwords.\n",
    "\n",
    "3. Vectorizar los documentos utilizando un modelo Word2Vec preentrenado\n",
    "de la librer ́ıa gensim. Indicar el modelo utilizado.\n",
    "\n",
    "4. Construir el tensor X llevando todos los documentos a un mismo largo\n",
    "(par ́ametro ajustable). Adem ́as, construir el vector y con la clasificaci ́on\n",
    "de cada documento incluido en X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Chilean primary healthcare practice is analyz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Attempting to deepen the understanding of fact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>The export of fresh blueberries is an importan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The initial microflora of minimally processed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>In the context of multidimensional structures,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>3</td>\n",
       "      <td>BackgroundLupin is a protein-rich legume with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>3</td>\n",
       "      <td>Phycobiliproteins, the main polypeptidic compo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>3</td>\n",
       "      <td>There is great interest in the food, cosmetic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>3</td>\n",
       "      <td>The use of vegetable proteins as food ingredie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>3</td>\n",
       "      <td>The spliceosome is a very dynamic cellular rib...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     department                                           abstract\n",
       "0             1   Chilean primary healthcare practice is analyz...\n",
       "1             1  Attempting to deepen the understanding of fact...\n",
       "2             1  The export of fresh blueberries is an importan...\n",
       "3             1  The initial microflora of minimally processed ...\n",
       "4             1  In the context of multidimensional structures,...\n",
       "..          ...                                                ...\n",
       "194           3  BackgroundLupin is a protein-rich legume with ...\n",
       "195           3  Phycobiliproteins, the main polypeptidic compo...\n",
       "196           3  There is great interest in the food, cosmetic ...\n",
       "197           3  The use of vegetable proteins as food ingredie...\n",
       "198           3  The spliceosome is a very dynamic cellular rib...\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "df=pd.read_csv(r'Abstracts.csv',header=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "semilla = 70\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=semilla, stratify=df['department'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocesador(text):\n",
    "    puntuacion = r'[,;.:•“”¡!¿?<>@#$%&[\\](){}<>~=+\\-*/|\\\\_^`\"\\']' #caracteres que no son abecedario\n",
    "    text = re.sub(puntuacion, ' ', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) #convierte a minuscula\n",
    "    text = re.sub(r'\\d','', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df_train['abstract'] = df_train['abstract'].apply(preprocesador)\n",
    "df_test['abstract'] = df_test['abstract'].apply(preprocesador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['abstract'].values.tolist()\n",
    "stop_words = nltk.corpus.stopwords.words('english') #elimina las stopwords\n",
    "tok_corp = [nltk.word_tokenize(sent) for sent in corpus]\n",
    "tok_corp = [x for x in tok_corp if x not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model = Word2Vec(tok_corp, vector_size=100, window=3, min_count=1, workers=8, sg=1, alpha=0.001, epochs=10)\n",
    "model.save('Word2VecSG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5660"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec.load(r'Word2VecSG')\n",
    "word_vectors = model.wv\n",
    "words = list(model.wv.key_to_index)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader \n",
    "\n",
    "#modelo mlp para hacer la vectorización\n",
    "wv = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'tópico'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5104\\3521025003.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mtokenizar_y_vectorizar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtok_corp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mvectorized_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jsepu\\miniconda3\\envs\\ima357_b\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5571\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5572\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5573\u001b[0m         ):\n\u001b[0;32m   5574\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5575\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'tópico'"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenizar_y_vectorizar(dataset):\n",
    "    corpus = dataset['abstract'].values.tolist()\n",
    "    tok_corp = [nltk.word_tokenize(sent) for sent in corpus]\n",
    "    vectorized_data = []\n",
    "    for sample in tok_corp:\n",
    "        vectors = [wv[w] for w in sample if w in wv.key_to_index]\n",
    "        vectorized_data.append(vectors)\n",
    "    return vectorized_data\n",
    "X = tokenizar_y_vectorizar(df)\n",
    "y = df.tópico.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta 2 (2 pts)\n",
    "Ajustar un modelo para Clasificar los Abstracts. Para esto:\n",
    "1. Separar el conjunto (X, y) de manera estratificada en un conjunto de en-\n",
    "trenamiento, con el 70% de los datos, y un conjunto de validaci ́on con los\n",
    "datos restantes. ¿Cu ́al es la dimensi ́on de los datos de entrada?. Interpre-\n",
    "tar los valores de la dimensi ́on.\n",
    "2. Entrenar una Red RNN para predecir el departamento asociado a cada\n",
    "abstract, considerando los conjuntos generados en el item anterior.\n",
    "3. Mostrar las curvas de aprendizaje del modelo durante el entrenamiento\n",
    "(para los conjuntos de entrenamiento y de validaci ́on), considerando la\n",
    "Funci ́on de P ́erdida y la Accuracy. Indicando si hay evidencia de sobrea-\n",
    "juste o bajoajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta 3 (2 ptos)\n",
    "Realizar una evaluaci ́on del modelo sobre datos desconocidos. Para esto:\n",
    "1. Generar un tensor llamado X test a partir de los datos del DataFrame\n",
    "que contiene al conjunto de test. Aplicar el mismo m ́etodo de preproce-\n",
    "samiento aplicado al conjunto de entrenamiento y utilizar el mismo modelo\n",
    "preentrenado Word2Vec y el mismo largo de los documentos.\n",
    "2. Utilizar el modelo entrenado en la pregunta 2 para predecir la clasificaci ́on\n",
    "de todos los datos incluidos en el tensor X test.\n",
    "3. Generar dos gr ́aficos de tortas: el primero para las clases reales y el se-\n",
    "gundo para las clases predichas por el modelo. Cada gr ́afico de torta debe\n",
    "mostrar las de clases asignando un color para cada clase. Agregar en cada\n",
    "gr ́afico la cantidad de datos de cada clase. Finalmente indicar la Accuracy\n",
    "obtenida por el modelo sobre los datos del conjunto test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regunta adicional (3 pts)\n",
    "1. Explorar las tareas asignadas hasta la pregunta 2 con varias configura-\n",
    "ciones diferentes de los par ́ametros de dise ̃no, a saber: largo de los doc-\n",
    "umentos, par ́ametros de entrenamiento (learning rate, batch, epocas) y\n",
    "par ́ametros de dise ̃no de la red (n ́umero de capas, tama ̃no de las capas\n",
    "intermedias, etc.). Considerar al menos 5 configuraciones diferentes de los\n",
    "par ́ametros.\n",
    "2. Determinar una combinaci ́on de par ́ametros, entre las configuraciones ex-\n",
    "ploradas, que genere los mejores resultados para la pregunta 2. Evidenciar\n",
    "esa comparaci ́on realizada.\n",
    "2\n",
    "3. Utilizar en la pregunta 3 la combinaci ́on optimal de par ́ametros encon-\n",
    "trada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ima357_b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
